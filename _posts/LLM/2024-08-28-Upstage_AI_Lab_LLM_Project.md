---
layout: single
title:  "Project_No.1_LLM"
categories: Project
tag: [Project]
toc: true
author_profile: false
---

<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }

    table.dataframe td {
      text-align: center;
      padding: 8px;
    }

    table.dataframe tr:hover {
      background: #b8d1f3; 
    }

    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 0.8rem !important;
  }

  </style>
</head>

## 1. .env í™˜ê²½ ì„¤ì •
ì²˜ìŒìœ¼ë¡œ .env í´ë”ì—ì„œ ì œê³µë°›ì€ Upstage_API_Keyë¥¼ ì…ë ¥í•˜ê³  ì €ì¥í•´ì¤€ë‹¤



```python
import os
from dotenv import load_dotenv

load_dotenv()

MAX_MESSAGES_BEFORE_DELETION = 5
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
```
## 2. chat_model.py
ë‹¤ìŒìœ¼ë¡œ chat_model.pyë¥¼ ë§Œë“¤ì–´ ì…ë ¥ê³¼ ì¶œë ¥ì„ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ì–´ì£¼ì—ˆë‹¤.



```python
# chat_model.py

from langchain_upstage import ChatUpstage
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from config import UPSTAGE_API_KEY

def setup_chat_model():
    chat = ChatUpstage(upstage_api_key=UPSTAGE_API_KEY)
    
    contextualize_q_system_prompt = """ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì´ ìˆì„ ë•Œ, ì´ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
    ì´ëŸ° ê²½ìš°, ëŒ€í™” ë‚´ìš©ì„ ì•Œ í•„ìš” ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”. 
    ì§ˆë¬¸ì— ë‹µí•  í•„ìš”ëŠ” ì—†ê³ , í•„ìš”í•˜ë‹¤ë©´ ê·¸ì € ë‹¤ì‹œ êµ¬ì„±í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”."""

    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])

    qa_system_prompt = """ì§ˆë¬¸-ë‹µë³€ ì—…ë¬´ë¥¼ ë•ëŠ” ë³´ì¡°ì›ì…ë‹ˆë‹¤. 
    ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ê²€ìƒ‰ëœ ë‚´ìš©ì„ ì‚¬ìš©í•˜ì„¸ìš”. 
    ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”. 
    ì¹œì ˆí•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•´ì„œ ë§í•˜ì„¸ì˜¤.
    10ì¤„ ì •ë„ë¡œ ì •ë¦¬í•´ì„œ ë§í•˜ì„¸ìš”.

    ## ë‹µë³€ ì˜ˆì‹œ
    ğŸ“ë‹µë³€ ë‚´ìš©: 
    ğŸ“ì¦ê±°(ì°¸ê³  ë¶€ë¶„): 

    {context}"""

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])

    return chat, contextualize_q_prompt, qa_prompt

def create_rag_chain(chat, retriever, contextualize_q_prompt, qa_prompt):
    history_aware_retriever = create_history_aware_retriever(chat, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(chat, qa_prompt)
    return create_retrieval_chain(history_aware_retriever, question_answer_chain)
```

## 3. doucument_loader.py
doucument_loader.pyë¥¼ ë§Œë“¤ì–´ PDF íŒŒì¼ ì—…ë¡œë“œê°€ ê°€ëŠ¥í•˜ë„ë¡ ë§Œë“¤ì—ˆë‹¤.



```python
from langchain_community.document_loaders import PyPDFLoader

def load_pdf(file_path):
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()
```

Vector DBë¡œ ì €ì¥í•˜ê¸° ìœ„í•´ Embeddingì„ ì§„í–‰í•˜ì˜€ë‹¤.



```python
from langchain_chroma import Chroma
from langchain_upstage import UpstageEmbeddings

def create_vectorstore(pages):
    vectorstore = Chroma.from_documents(pages, UpstageEmbeddings(model="solar-embedding-1-large"))
    return vectorstore.as_retriever(k=2)
```

## 4. ì‹œê°í™”ë¥¼ ìœ„í•œ streamlit
LLM ëª¨ë¸ì˜ ì…ë ¥ê³¼ ì¶œë ¥ ê·¸ë¦¬ê³  PDFíŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•œ ì‹œê°í™”ë¥¼ ìœ„í•´ streamlitì„ ì´ìš©í•˜ì—¬ ì§„í–‰í•˜ì˜€ë‹¤.



```python
import streamlit as st
import base64

def reset_chat():
    st.session_state.messages = []
    st.session_state.context = None

def display_pdf(file):
    st.markdown("### PDF Preview")
    base64_pdf = base64.b64encode(file.read()).decode("utf-8")
    pdf_display = f"""<iframe src="data:application/pdf;base64,{base64_pdf}" width="400" height="100%" type="application/pdf"
                        style="height:100vh; width:100%"
                    >
                    </iframe>"""
    st.markdown(pdf_display, unsafe_allow_html=True)

def setup_sidebar():
    with st.sidebar:
        st.header("ì„œë¥˜ë¥¼ ì—…ë¡œë“œ í•˜ì„¸ìš”.")
        uploaded_file = st.file_uploader("`.pdf` íŒŒì¼ì„ ê³ ë¥´ì„¸ìš”.", type="pdf")
    return uploaded_file

def display_chat_messages():
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

def get_user_input():
    return st.chat_input("Ask a question!")
```

## 5. main.py
ë§ˆì§€ë§‰ìœ¼ë¡œ ì „ì²´ ëª¨ë“ˆì„ êµ¬ë™í•˜ê¸° ìœ„í•´ main.pyë¥¼ ë§Œë“¤ì—ˆë‹¤.



```python
import streamlit as st
import uuid
import tempfile
import os
import time
from config import MAX_MESSAGES_BEFORE_DELETION
from document_loader import load_pdf
from embedding import create_vectorstore
from chat_model import setup_chat_model, create_rag_chain
from ui_components import reset_chat, setup_sidebar, display_chat_messages, get_user_input
def main():
    st.title("QA Engine 1íŒ€ ì±—ë´‡")
    if "id" not in st.session_state:
        st.session_state.id = uuid.uuid4()
        st.session_state.file_cache = {}
    if "messages" not in st.session_state:
        st.session_state.messages = []
    uploaded_file = setup_sidebar()
    if uploaded_file:
        try:
            file_key = f"{st.session_state.id}-{uploaded_file.name}"
            with tempfile.TemporaryDirectory() as temp_dir:
                file_path = os.path.join(temp_dir, uploaded_file.name)
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getvalue())
                st.sidebar.write("Indexing your document...")
                if file_key not in st.session_state.get('file_cache', {}):
                    pages = load_pdf(file_path)
                    retriever = create_vectorstore(pages)
                    chat, contextualize_q_prompt, qa_prompt = setup_chat_model()
                    rag_chain = create_rag_chain(chat, retriever, contextualize_q_prompt, qa_prompt)
                    st.session_state.file_cache[file_key] = rag_chain
                st.sidebar.success("Ready to Chat!")
        except Exception as e:
            st.sidebar.error(f"An error occurred: {e}")
            st.stop()
    display_chat_messages()
    if prompt := get_user_input():
        if len(st.session_state.messages) >= MAX_MESSAGES_BEFORE_DELETION:
            del st.session_state.messages[:2]
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            if uploaded_file:
                rag_chain = st.session_state.file_cache[file_key]
                result = rag_chain.invoke({"input": prompt, "chat_history": st.session_state.messages})
                with st.expander("Evidence context"):
                    st.write(result["context"])
                for chunk in result["answer"].split(" "):
                    full_response += chunk + " "
                    time.sleep(0.2)
                    message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)
            else:
                full_response = "Please upload a PDF file first."
                message_placeholder.markdown(full_response)
        st.session_state.messages.append({"role": "assistant", "content": full_response})
if __name__ == "__main__":
    main()
```
